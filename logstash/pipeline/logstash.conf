input {
  file {
    path => "/logs/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "plain"  # Read the logs as plain text first
  }
}

filter {
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:@timestamp} \[%{LOGLEVEL:log.level}\](?: \[RequestId: %{UUID:request_id}\])?: %{GREEDYDATA:message}"
    }
    remove_field => ["@version", "host"]
  }

  # Convert the parsed timestamp to the Logstash @timestamp field for time-based indexing in Kibana
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
    remove_field => ["timestamp"]  # Remove original timestamp field
  }

  # Add fields to standardize with ECS (Elastic Common Schema) if necessary
  mutate {
    add_field => {
      "log.level" => "%{log_level}"
      "event.dataset" => "application-logs"
    }
    remove_field => ["log_level"]
  }

  # Optional: Remove empty fields
  prune {
    whitelist_names => ["@timestamp", "message", "request_id", "log.level", "event.dataset"]
  }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "application-logs-%{+YYYY.MM.dd}"
  }
}

